{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701317560536,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "B6uxkeDhy8ww"
   },
   "outputs": [],
   "source": [
    "#The purpose of the preprocess_input function is to preprocess the input images before feeding them to an EfficientNet model\n",
    "from keras.applications.efficientnet import preprocess_input\n",
    "#This line imports the `EfficientNetB0` model from Keras' applications module\n",
    "from keras.applications import EfficientNetB0\n",
    "#This imports the `Sequential` model from Keras. A Sequential model is a stack of layers.\n",
    "#It's a basic form of neural network model in Keras where you can simply add layers to the model in sequence.\n",
    "#This is a \"linear\" arrangement in the sense that the data flows through the layers in a single path, without branching or merging.\n",
    "from keras.models import Sequential\n",
    "# - This line imports specific layers that you can add to your neural network:\n",
    "#     - `GlobalAveragePooling2D`: A layer that averages each feature map to a single number.\n",
    "#     - `Dropout`: This layer randomly sets a fraction of input units to 0 at each update during training\n",
    "from keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "#Keras might not have every possible metric or loss function you need and although the RMSE, for instance,\n",
    "#is a standard metric for regression problems, it is not directly available as a built-in function in Keras.\n",
    "#Keras does not provide \"low-level\" operations such as tensor multiplication and convolution.\n",
    "#Instead, it relies on the tensor library: `keras.backend` to handle these operations.\n",
    "#We are using that library to define our custom loss function.\n",
    "#By using `keras.backend`, your custom function remains compatible with whichever backend Keras is using.\n",
    "#This means you can write your code once and it will work whether you're using TensorFlow, Theano, or any other backend supported by Keras.\n",
    "# This allows for more flexible and portable code.\n",
    "import keras.backend as backend\n",
    "#ImageDataGenerator is a class in Keras used for real-time data augmentation.\n",
    "#'img_to_array' is a utility function that converts a loaded image (in the form of a PIL image or a similar object) into a NumPy array.\n",
    "#This conversion is necessary because deep learning models in Keras work with data in the form of NumPy arrays.\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "#The os module provides a way of using operating system-dependent functionality.\n",
    "#It allows you to interact with the operating system in various ways, such as navigating the file system, reading, and writing files,\n",
    "#querying and setting environment variables, and executing system commands.\n",
    "import os\n",
    "#cv2 provides tools that are essential for many tasks in the field of computer vision and image processing.\n",
    "import cv2\n",
    "#Import the ADAM optimizer\n",
    "from keras.optimizers import Adam\n",
    "#Remember that the Kaggle challenge includes a CSV file with the \"labels\" we are to predict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701317560536,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "Yi_e6L_EPgg4"
   },
   "outputs": [],
   "source": [
    "#The probability distributions for the classifications for each of the training images comes in a CSV file.\n",
    "#The columns of the CSV file are: \"GalxaxyID, Class1.1, Class1.2, ...\"\n",
    "classes = [\n",
    "    'Class1.1', 'Class1.2', 'Class1.3', 'Class2.1', 'Class2.2', 'Class3.1',\n",
    "    'Class3.2', 'Class4.1', 'Class4.2', 'Class5.1', 'Class5.2', 'Class5.3',\n",
    "    'Class5.4', 'Class6.1', 'Class6.2', 'Class7.1', 'Class7.2', 'Class7.3',\n",
    "    'Class8.1', 'Class8.2', 'Class8.3', 'Class8.4', 'Class8.5', 'Class8.6',\n",
    "    'Class8.7', 'Class9.1', 'Class9.2', 'Class9.3', 'Class10.1', 'Class10.2',\n",
    "    'Class10.3', 'Class11.1', 'Class11.2', 'Class11.3', 'Class11.4',\n",
    "    'Class11.5', 'Class11.6'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701317560537,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "wiT_zf9LPkNb"
   },
   "outputs": [],
   "source": [
    "#This line sets the variable DIR to the path of the CSV file provided by the Kaggle challenge, this\n",
    "#CSV file contains the entries of the \"classification\" vectors for each galaxy in the training set\n",
    "#DIR = \"/content/drive/MyDrive/DL_project/training_solutions_rev1.csv\"\n",
    "DIR = 'C:/Users/usuario/Desktop/DL_project_Jupyter/training_solutions_rev1.csv'\n",
    "\n",
    "#This line sets the variable train_path to the path of a directory that contains the training images\n",
    "#train_path = \"/content/drive/Othercomputers/Mi portátil/images_training_rev1\"\n",
    "train_path = 'C:/Users/usuario/Desktop/DL_project_Jupyter/images_training_rev1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701317560537,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "4yv46l9oPx3V"
   },
   "outputs": [],
   "source": [
    "#This function takes a single argument fn (short for \"filename\") and returns the filename with \".jpg\" appended to it\n",
    "#This function we are going to use in order to read the images\n",
    "def append_ext(fn):\n",
    "    return fn + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1701317561562,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "MDEESjUXP5bG"
   },
   "outputs": [],
   "source": [
    "#We read the dataset whose path is stored by the \"DIR\" variable\n",
    "traindf = pd.read_csv(DIR)\n",
    "#We take the 'GalaxyID' column of the DataFrame, convert it to string type (using .astype(str)),\n",
    "#and then applies the append_ext function to each element in the column.\n",
    "#This is done using the .apply() method, which applies a function along an axis of the DataFrame.\n",
    "#The result is that each entry in the new 'id' column is the corresponding GalaxyID with \".jpg\" appended to it.\n",
    "#hence, \"id\" will contain the filename of each image\n",
    "traindf[\"id\"] = traindf['GalaxyID'].astype(str).apply(append_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1701317561582,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "sGDsplyOaXaG"
   },
   "outputs": [],
   "source": [
    "#The cropping we explained above\n",
    "def random_input(img):\n",
    "    #By using [:2], we ignore the number of channels and keep just the height and width.\n",
    "    shape = img.shape[:2]\n",
    "    #These lines calculate one-fourth of the height and width of the image, respectively.\n",
    "    left = int(shape[0]/4)\n",
    "    top = int(shape[1]/4)\n",
    "    #This line crops the image to a central region.\n",
    "    #It selects a square from the image that starts at (left, top) and extends to three times the value of left and top.\n",
    "    img = img[left:left*3,top:top*3,:]\n",
    "    #After cropping, the image is resized back to its original dimensions\n",
    "    #interpolation=cv2.INTER_CUBIC argument specifies the interpolation method to be cubic, which is a method that generally provides good results.\n",
    "    image = cv2.resize(img, shape, interpolation = cv2.INTER_CUBIC)\n",
    "    #This line converts the resized image into a NumPy array using the img_to_array function.\n",
    "    #This conversion is necessary because Keras models expect input in the form of NumPy arrays.\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    #Apply EfficientNetB0 preprocess_input\n",
    "    return preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1701317561583,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "LE7awRPFj_GE"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    #This parameter specifies that each image will be randomly rotated by a degree between -90 and +90.\n",
    "    rotation_range=90,\n",
    "    #These parameters allow for random horizontal and vertical shifts of the image.\n",
    "    #`width_shift_range=0.1` means the image will be shifted horizontally by up to 10% of its width.\n",
    "    #Similarly, `height_shift_range=0.1` allows for up to 10% vertical shift.\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    #This parameter randomly changes the brightness of the image. The brightness will be adjusted by a factor chosen from the range [0.9, 1.2]\n",
    "    #Since pixel values must be within the range 0 to 255, if the multiplication results in a value outside this range, it will be clipped to fit within it.\n",
    "    brightness_range = (0.9, 1.2),\n",
    "    #These parameters enable random flipping of the images horizontally and vertically.\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    #This parameter is used to reserve a portion of the images for validation. In this case, 15% (`0.15`) of the images will be used for validation.\n",
    "    validation_split = 0.15,\n",
    "    # Here, you can specify a custom preprocessing function to apply to each image after the augmentation and before feeding it to the model.\n",
    "    preprocessing_function = random_input,\n",
    ")\n",
    "\n",
    "#Setting the validation_split parameter in both ImageDataGenerator instances\n",
    "#(datagen for training data and valid_datagen for validation data) is a common practice\n",
    "#when you have a single dataset and you want to split it into training and validation sets automatically.\n",
    "#You will understand this once you review the \"flow_from_dataframe\" class, there you will see that\n",
    "#each ImageDataGenerator takes care of generating either the training set or the validation set\n",
    "#I need to specify the percentages for both given that there is no a priori communcation between them\n",
    "\n",
    "#We an instance of the ImageDataGenerator class from Keras, which is specifically configured for generating validation data.\n",
    "valid_datagen=ImageDataGenerator(validation_split=0.15, preprocessing_function = random_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1701317561584,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "XhJo6JXdG_TO",
    "outputId": "5765c52b-1afd-4e08-c698-e8ea04cce6f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GalaxyID</th>\n",
       "      <th>Class1.1</th>\n",
       "      <th>Class1.2</th>\n",
       "      <th>Class1.3</th>\n",
       "      <th>Class2.1</th>\n",
       "      <th>Class2.2</th>\n",
       "      <th>Class3.1</th>\n",
       "      <th>Class3.2</th>\n",
       "      <th>Class4.1</th>\n",
       "      <th>Class4.2</th>\n",
       "      <th>...</th>\n",
       "      <th>Class10.1</th>\n",
       "      <th>Class10.2</th>\n",
       "      <th>Class10.3</th>\n",
       "      <th>Class11.1</th>\n",
       "      <th>Class11.2</th>\n",
       "      <th>Class11.3</th>\n",
       "      <th>Class11.4</th>\n",
       "      <th>Class11.5</th>\n",
       "      <th>Class11.6</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100008</td>\n",
       "      <td>0.383147</td>\n",
       "      <td>0.616853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616853</td>\n",
       "      <td>0.038452</td>\n",
       "      <td>0.578401</td>\n",
       "      <td>0.418398</td>\n",
       "      <td>0.198455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279952</td>\n",
       "      <td>0.138445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325512</td>\n",
       "      <td>100008.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100023</td>\n",
       "      <td>0.327001</td>\n",
       "      <td>0.663777</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.031178</td>\n",
       "      <td>0.632599</td>\n",
       "      <td>0.467370</td>\n",
       "      <td>0.165229</td>\n",
       "      <td>0.591328</td>\n",
       "      <td>0.041271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131378</td>\n",
       "      <td>0.459950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100023.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100053</td>\n",
       "      <td>0.765717</td>\n",
       "      <td>0.177352</td>\n",
       "      <td>0.056931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100053.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100078</td>\n",
       "      <td>0.693377</td>\n",
       "      <td>0.238564</td>\n",
       "      <td>0.068059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238564</td>\n",
       "      <td>0.109493</td>\n",
       "      <td>0.129071</td>\n",
       "      <td>0.189098</td>\n",
       "      <td>0.049466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094549</td>\n",
       "      <td>0.189098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100078.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100090</td>\n",
       "      <td>0.933839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100090.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61573</th>\n",
       "      <td>999948</td>\n",
       "      <td>0.510379</td>\n",
       "      <td>0.489621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.430414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430414</td>\n",
       "      <td>0.226257</td>\n",
       "      <td>0.204157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226257</td>\n",
       "      <td>999948.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61574</th>\n",
       "      <td>999950</td>\n",
       "      <td>0.901216</td>\n",
       "      <td>0.098784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999950.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61575</th>\n",
       "      <td>999958</td>\n",
       "      <td>0.202841</td>\n",
       "      <td>0.777376</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>0.116962</td>\n",
       "      <td>0.660414</td>\n",
       "      <td>0.067245</td>\n",
       "      <td>0.593168</td>\n",
       "      <td>0.140022</td>\n",
       "      <td>0.520391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090673</td>\n",
       "      <td>0.049349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072296</td>\n",
       "      <td>999958.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61576</th>\n",
       "      <td>999964</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045450</td>\n",
       "      <td>0.863550</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.841098</td>\n",
       "      <td>0.795330</td>\n",
       "      <td>0.068220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068398</td>\n",
       "      <td>0.318132</td>\n",
       "      <td>0.408799</td>\n",
       "      <td>0.227464</td>\n",
       "      <td>0.408799</td>\n",
       "      <td>0.090668</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045334</td>\n",
       "      <td>999964.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61577</th>\n",
       "      <td>999967</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.116620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>999967.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61578 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GalaxyID  Class1.1  Class1.2  Class1.3  Class2.1  Class2.2  Class3.1  \\\n",
       "0        100008  0.383147  0.616853  0.000000  0.000000  0.616853  0.038452   \n",
       "1        100023  0.327001  0.663777  0.009222  0.031178  0.632599  0.467370   \n",
       "2        100053  0.765717  0.177352  0.056931  0.000000  0.177352  0.000000   \n",
       "3        100078  0.693377  0.238564  0.068059  0.000000  0.238564  0.109493   \n",
       "4        100090  0.933839  0.000000  0.066161  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "61573    999948  0.510379  0.489621  0.000000  0.059207  0.430414  0.000000   \n",
       "61574    999950  0.901216  0.098784  0.000000  0.000000  0.098784  0.000000   \n",
       "61575    999958  0.202841  0.777376  0.019783  0.116962  0.660414  0.067245   \n",
       "61576    999964  0.091000  0.909000  0.000000  0.045450  0.863550  0.022452   \n",
       "61577    999967  0.767000  0.140000  0.093000  0.000000  0.140000  0.000000   \n",
       "\n",
       "       Class3.2  Class4.1  Class4.2  ...  Class10.1  Class10.2  Class10.3  \\\n",
       "0      0.578401  0.418398  0.198455  ...   0.279952   0.138445   0.000000   \n",
       "1      0.165229  0.591328  0.041271  ...   0.000000   0.131378   0.459950   \n",
       "2      0.177352  0.000000  0.177352  ...   0.000000   0.000000   0.000000   \n",
       "3      0.129071  0.189098  0.049466  ...   0.094549   0.000000   0.094549   \n",
       "4      0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "...         ...       ...       ...  ...        ...        ...        ...   \n",
       "61573  0.430414  0.226257  0.204157  ...   0.226257   0.000000   0.000000   \n",
       "61574  0.098784  0.000000  0.098784  ...   0.000000   0.000000   0.000000   \n",
       "61575  0.593168  0.140022  0.520391  ...   0.000000   0.090673   0.049349   \n",
       "61576  0.841098  0.795330  0.068220  ...   0.068398   0.318132   0.408799   \n",
       "61577  0.140000  0.023380  0.116620  ...   0.023380   0.000000   0.000000   \n",
       "\n",
       "       Class11.1  Class11.2  Class11.3  Class11.4  Class11.5  Class11.6  \\\n",
       "0       0.000000   0.092886   0.000000   0.000000        0.0   0.325512   \n",
       "1       0.000000   0.591328   0.000000   0.000000        0.0   0.000000   \n",
       "2       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "3       0.189098   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "4       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "61573   0.000000   0.000000   0.000000   0.000000        0.0   0.226257   \n",
       "61574   0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "61575   0.000000   0.067726   0.000000   0.000000        0.0   0.072296   \n",
       "61576   0.227464   0.408799   0.090668   0.023065        0.0   0.045334   \n",
       "61577   0.000000   0.000000   0.000000   0.000000        0.0   0.023380   \n",
       "\n",
       "               id  \n",
       "0      100008.jpg  \n",
       "1      100023.jpg  \n",
       "2      100053.jpg  \n",
       "3      100078.jpg  \n",
       "4      100090.jpg  \n",
       "...           ...  \n",
       "61573  999948.jpg  \n",
       "61574  999950.jpg  \n",
       "61575  999958.jpg  \n",
       "61576  999964.jpg  \n",
       "61577  999967.jpg  \n",
       "\n",
       "[61578 rows x 39 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261078,
     "status": "ok",
     "timestamp": 1701317822631,
     "user": {
      "displayName": "Luis Mario Nunez Beltran",
      "userId": "06293216306995136507"
     },
     "user_tz": 360
    },
    "id": "EcSxs3I-As8L",
    "outputId": "ca1ac467-0d25-4973-c2a5-39a55cc70d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52342 validated image filenames.\n",
      "Found 9236 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "#We create an instance of a data generator for training data using the flow_from_dataframe method of the previously defined ImageDataGenerator instance (datagen).\n",
    "\n",
    "#The dataframe parameter specifies the pandas DataFrame (traindf) that contains our training data.\n",
    "#This DataFrame should have two columns: one for the image identifiers and one for the labels.\n",
    "\n",
    "#directory specifies the path to the directory where the images are stored (train_path).\n",
    "#The generator will look for the images mentioned in the traindf DataFrame in this directory.\n",
    "\n",
    "#x_col specifies the name of the column in traindf that contains the image file names\n",
    "\n",
    "#y_col specifies the column or columns that contain the labels. Since we are working on a multi-label classification problem\n",
    "#this parameter should be set to a list of column names in traindf with the different class names.\n",
    "\n",
    "#subset is used to specify which part of the data to use. By setting it to \"training\", you indicate that\n",
    "#the generator should use the part of the data designated for training (based on the validation_split defined in the ImageDataGenerator).\n",
    "\n",
    "#batch_size=70: This sets the size of the batches of data to be generated. Each batch will contain 70 images (and their corresponding labels).\n",
    "\n",
    "#The seed parameter sets the random seed for shuffling and transformations, ensuring reproducibility of your batches.\n",
    "\n",
    "#No shuffle is applied, if you think about it, it is not necessary for us.\n",
    "\n",
    "#The class_mode parameter specifies how the labels are represented. Setting it to \"raw\" means that the labels will be provided as they are in the DataFrame.\n",
    "#For instance, if class_mode=\"categorical\", and you have images classified into 'cat', 'dog', and 'bird',\n",
    "#the labels will be converted into one-hot encoded format, like [1, 0, 0] for 'cat', [0, 1, 0] for 'dog', and [0, 0, 1] for 'bird'.\n",
    "\n",
    "#target_size sets the dimensions to which all images found will be resized. In this case, images will be resized to 224x224 pixels.\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=train_path,\n",
    "    x_col=\"id\",\n",
    "    y_col=classes,\n",
    "    subset=\"training\",\n",
    "    batch_size=70,\n",
    "    seed=123,\n",
    "    shuffle=False,\n",
    "    class_mode=\"raw\",\n",
    "    target_size=(224,224))\n",
    "\n",
    "\n",
    "#We do something analogous for the valid_generator class\n",
    "valid_generator = valid_datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=train_path,\n",
    "    x_col=\"id\",\n",
    "    y_col=classes,\n",
    "    subset=\"validation\",\n",
    "    batch_size=70,\n",
    "    seed=123,\n",
    "    shuffle=False,\n",
    "    class_mode=\"raw\",\n",
    "    target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c4G4Y6WAAs-3"
   },
   "outputs": [],
   "source": [
    "#The following line calculates the number of steps (or batches) per epoch during the training process.\n",
    "#The .n attribute of a Keras data generator gives you the total number of images (or samples) in the dataset that the generator is drawing from.\n",
    "#train_generator.batch_size is the number of samples that will be processed in each batch.\n",
    "#The // operator in Python performs integer (or floor) division\n",
    "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "\n",
    "#And we do something analogous for the validation set\n",
    "STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5e2HRv9L4Op1"
   },
   "outputs": [],
   "source": [
    "#The Kaggle Galaxy Zoo challenge asked participants to use the Root Mean Square Error (RMSE) of their predictions to evaluate the performance of their models:\n",
    "def rmse(y_true, y_pred):\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HQ_bj4stuALI"
   },
   "outputs": [],
   "source": [
    "#Here we construct a neural network model using Keras utilizing the EfficientNetB0 architecture as the base model.\n",
    "def build_model():\n",
    "    #This line initializes an EfficientNetB0 model pre-trained on the ImageNet dataset.\n",
    "    #weights='imagenet' indicates that the model should be loaded with weights trained on the ImageNet dataset.\n",
    "    #include_top=False means that the top layer of the model (a fully connected layer for classification) is not included. This allows for custom layers to be added for specific tasks.\n",
    "    #input_shape=(224, 224, 3) sets the shape of the input images to 224x224 pixels with 3 color channels (RGB).\n",
    "    eff1 = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    #This \"freezes\" these layers, meaning their weights will not be updated during training.\n",
    "    #This is a common practice when using a pre-trained model as a feature extractor,\n",
    "    #as it allows the model to maintain the knowledge it has gained from the original training dataset (ImageNet in this case).\n",
    "    for layer in eff1.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #model = Sequential(): This line initializes a new Sequential model.\n",
    "    model = Sequential()\n",
    "    #model.add(eff1): Adds the EfficientNetB0 model as the base of the new model.\n",
    "    model.add(eff1)\n",
    "    #model.add(GlobalAveragePooling2D()): This layer applies global average pooling to the output of EfficientNetB0.\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    #model.add(Dropout(0.5)): This layer randomly sets input units to 0 with a frequency of 0.5 at each instance (image) during training.\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(64, activation='relu')): Adds a densely-connected Neural Network layer with 64 units and ReLU (Rectified Linear Unit) activation.\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    #model.add(Dense(37, activation='sigmoid')): Finally, adds a Dense layer with 37 units and a sigmoid activation function.\n",
    "    model.add(Dense(37, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4ICVVYSS5A-Y"
   },
   "outputs": [],
   "source": [
    "#The line model = build_model() is calling the build_model function that you defined earlier and storing the returned model in the variable model\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the weights from the last checkpoint\n",
    "model.load_weights('C:/Users/usuario/Desktop/DL_project_Jupyter/weights_efficientnetB0_justClassifiers.hdf5')\n",
    "\n",
    "#When you load the weights with model.load_weights(), it only affects the weights of the layers, \n",
    "#not their trainable status. So the pre-trained EfficientNetB0 base will remain frozen, \n",
    "#and only the weights of the classifier layers at the end (which were trained by you) will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the EfficientNetB0 layers for full model training\n",
    "\n",
    "#model.layers[0]: This retrieves the first layer of the Sequential model. \n",
    "#Since the first thing you added to your Sequential model was the EfficientNetB0 model, \n",
    "#model.layers[0] is the EfficientNetB0 model itself.\n",
    "\n",
    "for layer in model.layers[0].layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "#You have to do it this way given that so far we have just saved the \"best weights\" for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0JTkJKlp6Z2l"
   },
   "outputs": [],
   "source": [
    "#This function configures the model with a loss function, an optimizer, and one or more metrics for evaluation.\n",
    "#In our case, the rmse function defined above is used as the loss function\n",
    "#We are using the Adam optimizer, lr=1.5e-4 sets the learning rate of the Adam optimizer to 0.00015.\n",
    "#Metrics are used to evaluate the performance of your model. They are similar to the loss function but are not used for training the model, only for evaluation.\n",
    "model.compile(loss=rmse,\n",
    "                  optimizer=Adam(learning_rate=1.5e-4),\n",
    "                  metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yllrP7jzUryB"
   },
   "outputs": [],
   "source": [
    "#A callback is a function that is passed into another function or class as an argument and is expected to be executed when certain event happens.\n",
    "#Callback classes are an extension of this concept.\n",
    "#They are classes that define a set of methods which are called at specific points during the execution of a program or in response to certain events.\n",
    "#In deep learning, callback classes are used extensively to monitor and influence the training process of models.\n",
    "from keras.callbacks import Callback\n",
    "#This line imports various callback classes from Keras.\n",
    "#Callbacks like ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, and CSVLogger provide functionalities to\n",
    "#save the model at certain points, stop training early under certain conditions, reduce the learning rate when a metric has stopped improving,\n",
    "#and log the training process to a CSV file, respectively.\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "#The LossHistory class is an example of a custom callback class.\n",
    "#This class is designed to track and record the loss, validation loss, and RMSE after each batch during the training of a neural network.\n",
    "#This class extends Keras's Callback base class, by inheriting from it\n",
    "class LossHistory(Callback):\n",
    "    #This method is automatically called at the beginning of the training process (that is built in the Keras callback class).\n",
    "    #Inside this method, three empty lists (self.losses, self.val_losses, and self.rmse) are initialized.\n",
    "    #These lists will be used to store the values of the training loss, validation loss, and RMSE, respectively, as the training progresses.\n",
    "    #It is not clear to me what the purpose of receiving the \"logs\" dictionary here\n",
    "    #the logs dictionary will contain the current training metrics.\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.rmse = []\n",
    "\n",
    "    #This method is executed at the end of each batch during the training process.\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        #The method extracts the current batch's loss, val_loss, and rmse values from the\n",
    "        #logs dictionary and appends them to the corresponding lists (self.losses, self.val_losses, and self.rmse).\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.rmse.append(logs.get('rmse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Nj6CtmVGY_U5"
   },
   "outputs": [],
   "source": [
    "#This line creates an instance of the EarlyStopping callback.\n",
    "#The purpose of this callback is to stop the training process early if there is no improvement in a specified metric after a certain number of epochs (patience).\n",
    "\n",
    "#monitor='val_loss' specifies that the callback should monitor the validation loss.\n",
    "\n",
    "#patience=10 means that the training will be stopped if there is no improvement in the monitored metric for 10 consecutive epochs.\n",
    "#\"No improvement\" is defined based on the mode parameter.\n",
    "\n",
    "#verbose=1 enables verbose output. This means the callback will print a message when the training is stopped early.\n",
    "\n",
    "#mode='auto' lets the callback decide the appropriate mode for monitoring (min or max) based on the monitored metric.\n",
    "#For val_loss, it will automatically set the mode to min, meaning training will stop when the metric stops decreasing.\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "\n",
    "#This line creates an instance of the LossHistory callback, which you defined earlier.\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BK6vHIZUY_XY"
   },
   "outputs": [],
   "source": [
    "#The ModelCheckpoint callback automatically saves the model or model weights at specified intervals during training.\n",
    "\n",
    "#filepath specifies the location to save the model file.\n",
    "\n",
    "#verbose=2 enables verbose output. It will print a message each time the model is saved, but the message is shorter than for verbose=1\n",
    "\n",
    "#save_best_only=True means the model will only be saved when the monitored metric (which is val_loss) has improved.\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='C:/Users/usuario/Desktop/DL_project_Jupyter/weights_efficientnetB0.hdf5', verbose=2, save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "#This callback reduces the learning rate when a metric has stopped improving.\n",
    "#Reducing the learning rate can help the model to converge to a minimum.\n",
    "\n",
    "#factor=0.2 means that the new learning rate will be 20% of the current learning rate when a reduction is made.\n",
    "\n",
    "#patience=4 means that the learning rate will be reduced if there is no improvement in the monitored metric for 4 consecutive epochs.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=4,)\n",
    "\n",
    "#The CSVLogger callback streams the results of each epoch into a CSV file\n",
    "csv_logger = CSVLogger('C:/Users/usuario/Desktop/DL_project_Jupyter/weights_efficientnetB0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Wqj3aKtN36b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Temp/ipykernel_10192/4199154999.py:23: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "#This is where the training of a deep learning model takes place using Keras\n",
    "\n",
    "#This .fit_generator method is used to train the model on data generated batch-by-batch by a Python generator (train_generator).\n",
    "#The fit_generator method is often used when the dataset is too large to fit into memory, requiring real-time data loading and augmentation.\n",
    "\n",
    "#The first argument is the training data generator (train_generator) that yields batches of training data and labels.\n",
    "\n",
    "#steps_per_epoch specifies the number of batch steps before declaring one epoch finished and starting the next epoch.\n",
    "#It is set to STEP_SIZE_TRAIN, which is the total number of training samples divided by the batch size.\n",
    "#This ensures that the model sees all training samples in each epoch.\n",
    "\n",
    "#The validation_data parameter is set to valid_generator, which is a similar generator but for validation data.\n",
    "#It provides batches of validation data and labels for evaluating the model's performance on data it has not trained on\n",
    "\n",
    "#validation_steps determines how many batches of validation data will be used in evaluating the model’s performance at the end of each epoch.\n",
    "#It is typically set (and this is our case) to the number of validation samples divided by the validation batch size.\n",
    "\n",
    "#epochs: this sets the number of epochs for which the training will run. The model will go through the training dataset 50 times.\n",
    "\n",
    "#callbacks are a set of functions applied at given stages of the training procedure.\n",
    "#You can use callbacks to get a view of some internal states and statistics of the model during training\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=50,\n",
    "    callbacks=[history, checkpointer, reduce_lr, early_stopping, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsDRTBTnUl2D6t0irDovbd",
   "provenance": [
    {
     "file_id": "1cwMtXfV7ORmGALR1XFqXwkTXpzrMq1he",
     "timestamp": 1701206153140
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
